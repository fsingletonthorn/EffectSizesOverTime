---
title: "Effect Sizes Over Time"
author: "Felix Singleton Thorn"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

# Setting n sig digits and hiding table NAs 
options(scipen=1, digits=3, knitr.kable.NA = '')

# Custom functions: 
# This just tables the output of meta-analyses nicely
niceMLMESum2 <- function(REMod) {
  data_frame(" " = c("Intercept", "Year",rep(NA,3)), 
                        Estimate = c(REMod$b[1], REMod$b[2], rep(NA, 3)), "95% CI LB" = c(REMod$ci.lb, rep(NA, 3)), "95% CI UB" = c(REMod$ci.ub, rep(NA, 3)), SE = c(REMod$se,  rep(NA, 3)), p = c(ifelse(REMod$pval<.001, "< .001", round(REMod$pval,2)),  rep(NA, 3)), 
             "Random effects" = c(NA, NA, paste0("Journal variance = ", round(REMod$sigma2[1], 3), ", n = ", 
                                             REMod$s.nlevels[1]),
                                  paste0("Article variance = ", round(REMod$sigma2[2], 3), ", n = ", REMod$s.nlevels[2]), 
                                  paste0("QE(",REMod$k-1, ") = ", round(REMod$QE, 2),  ", p ", ifelse(REMod$QEp <.001, "< .001", paste("=" , round(REMod$QEp, 2))))))
}

# This converts effect sizes from t, F, r and chi squared analyses to correlation coefficients (df2 = df for all but F tests)
# Adapted from https://osf.io/z7aux/ - Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251).  Retrieved from http://science.sciencemag.org/content/349/6251/aac4716.abstract
esComp <- function(x,df1,df2,esType) {
  esComp <- ifelse(esType=="t",sqrt((x^2*(1 / df2)) / (((x^2*1) / df2) + 1)), 
                  ifelse(esType=="F",sqrt((x*(df1 / df2)) / (((x*df1) / df2) + 1))*sqrt(1/df1),
                         ifelse(esType=="r",x,
                                ifelse(esType=="Chi2",sqrt(x/(df2 + 2)),
                                                     ifelse(esType == "Z",NA,NA)))))
  return(esComp)
}

```


```{r data import}
# importing the data
library(readr); library(EnvStats); library(cowplot); library(tidyverse); library(metafor); library(knitr); library(nlme)
data <- read_csv2(file = "data/150211FullFile_AllStatcheckData_Automatic1Tail.csv")

# exluding all articles where the articles are not appliable (i.e., samples don't go back to 1985)
includedJournals <- c("DP", "JAP", "JCCP", "JEPG", "JPSP")

dat <- data[data$journals.jour. %in% includedJournals,]

# Renamming for clarity
names(dat)[18] <- "journal"
names(dat)[19] <- "year"
names(dat)[1] <- "id"
names(dat)[2] <- "article"

```


```{r data cleaning}
# calculating pseudo correlation coefficents
dat$r <- esComp(x = dat$Value, df1 = dat$df1, df2 =  dat$df2, esType = dat$Statistic)

# setting to positive
dat$r <- abs(dat$r)

# Removing impossible rs 
# dat$Raw[dat$r > 1 & !is.na(dat$r)]
greaterthan1 <- sum(dat$r > 1 & !is.na(dat$r))

# removing impossible correlations 
dat$r[dat$r > 1 & !is.na(dat$r)] <- NA

# transforming to Fisher z
dat$z <- 0.5*log((1 + dat$r) / (1 - dat$r)) 

# Standard error for z (sample size taken as df2 - 2, se as sqrt(1/(sampleSize-3))
dat$SEz <- sqrt(1/(dat$df2 - 5))

# Binary for valid standard error
dat$vaidSE <- !((dat$Statistic=="Chi2" | (dat$Statistic=="F" & dat$df1 > 1) | (dat$Statistic == "Z")) | dat$df2 < 6 | is.na(dat$z) | dat$z == Inf)

# Extracting different subsets
datMeta <- filter(dat, dat$vaidSE)

datComplete <- dat[!is.na(dat$z)& !(dat$z == Inf) &!is.na(dat$SEz) & !(dat$SEz == Inf),]

# calculating the number of cases in which there were ties for the largest effect in a paper
nRepLargest <- nrow(dat %>%   filter(vaidSE) %>%   group_by(article) %>%  filter(z == max(z))) - nrow(dat %>%   filter(vaidSE) %>%   group_by(article) %>%  filter(z == max(z)) %>%  slice(1))

# write_rds(datMeta, "data/datMeta.RDS")

# Filtering for largest effect in each paper (if this changes, change nRepLargest code above)
datMetaLargest <- dat %>% 
  filter(vaidSE) %>% 
  group_by(article) %>%
  filter(z == max(z)) %>%
  slice(1)

# write_rds(datMetaLargest, "data/datMetaLargest.RDS")

# Filtering for first analysis in each paper
datMetaFirst <- dat %>% 
  filter(vaidSE) %>% 
  group_by(article) %>%
  slice(1)

```


```{r}
# Reading in data from models (model specification is below)
mod <- read_rds("data/mod.RDS")
modLargest <- read_rds("data/modLargest.RDS")
modFirst <- read_rds("data/modFirst.RDS")

changeOverTime <-  transf.ztor(mod$coefficients$fixed[1] + (mod$coefficients$fixed[2] * (2013 - mean(datMeta$year)))) - transf.ztor(mod$coefficients$fixed[1] + (mod$coefficients$fixed[2] * (1985 - mean(datMeta$year))))

# largest effect model's estimated difference between 1985 and today
changeOverTimeLargest <- transf.ztor(predict(modLargest, (2013 - mean(datMetaLargest$year)))[[1]])- transf.ztor(predict(modLargest, (1985 - mean(datMetaLargest$year)))[[1]])

```

## Abstract
This article uses mixed effects meta-regression to estimate the change in average effect sizes in psychological research using a database of over 130,000 effect size estimates from over 9,000 articles published in 5 APA journals from 1985 to 2013 {Nuijten, 2015 #550}. The results of this analysis suggest that the average effect size reported in psychological research is decreasing by `r mod$coefficients$fixed[2]` (95% CI [`r intervals(mod)$fixed[2,1]`, `r intervals(mod)$fixed[2,3]`]) Fisher Z score units per year. This represents a substantial decrease of `r changeOverTime` correlation coefficient units from 1985 to 2013.  Several exploratory analyses were also performed to investigate whether this change is also seen in subsets of reported effects that may be more likely to represent the effects of substantive interest; the first reported effect and the largest effect in each paper. Analysing just the first detected effect size in each paper  showed broad agreement with the main analysis (a `r modFirst$b[2]` estimated yearly change, 95% CI [`r modFirst$ci.lb[2]`, `r modFirst$ci.ub[2]`]), however, looking at the largest effect reported in each paper showed a slight increase in effect sizes over time (an estimated yearly change of `r options(digits=4); modLargest$b[2]`, 95% CI [`r modLargest$ci.lb[2]`,  `r modLargest$ci.ub[2]`])`r options(digits=3)`. Together these results suggest that there has been a small decrease in the average effect sizes reported in psychology over time, although the degree to which this decrease is reflective of a decrease in the size of the focal or main effects under study in psychology remains an open question.

## Introduction
An important question in understanding the history of psychological science is whether and how the magnitudes of the effects under study have changed over time. Are we studying smaller effects over time, having already found the “low hanging fruit” {Baumeister, 2016 #1022}? Or could methodological reforms and the increasing awareness of the importance of measurement in research design lead to increased effect sizes {Nelson, 2018 #750}? This paper uses an extensive database of over 130,000 effect size estimates from almost 10,000 articles published in 5 APA journals from 1985 to 2013 collected as part of {Nuijten, 2015 #550} to examine how effect sizes have changed in psychological research over this period.

The question of how effect sizes have changed over time is both of intrinsic interest and has important implications for understanding the results of surveys of the statistical power of  psychological research (e.g., {Cohen, 1962 #487;Rosnow, 1989 #37;Szucs, 2017 #25}). Most of the efforts to estimate the statistical power of psychological research have used Cohen's effect size benchmarks {Cohen, 1988 #562}, and as such any comparison of these studies over time assumes that effect sizes have been stable. According to our recent meta-analysis 46 power surveys including over 8,000 individual studies published from 1932-2014 [cite meta-analysis], the average statistical power of psychological research is  .23 95% CI [.17, .29]  for "small" effect sizes (effect sizes equivalent to r = .1 or Cohen's d = .2), .62 95% CI [.55, .69] to detect medium effects (r = .3 or Cohen's d = 0.5)), and .80, 95% CI [.68, .92] to detect large effects (effects equivalent to 0.8 Cohen's d or r = .5) [cite meta-analysis]. This same analysis also suggests that there has been little-to-no change in the average statistical power of research conducted in psychology to detect these effect size benchmarks over time. However, in order to know whether and how the average statistical power of psychological research has changed, it is necessary to know whether the effect sizes under study in psychological research have remained stable.

Previously, Monsarrat and Vergnes (2017) {Monsarrat, 2017 #1060} observed a field wide decline effect in the medical literature. Their study looked at effect sizes reported as odds ratios (including odds ratios, risk ratios and hazard ratios) in abstracts in the PubMed database and showed that these effects are, on average, decreasing over time. However, it is unclear whether this trend is also seen in the psychology literature. A small number of previous studies have extracted effect size benchmarks from psychological research in domains as varied as social {Richard, 2003 #603}, management {Paterson, 2015 #817; Bosco, 2015 #157} and clinical psychology {Haase, 1982 #516}. Estimates of the average effect size in various subfields range from an mean correlation coefficient of .21 in social psychology meta-analyses {Richard, 2003 #603}, to a mean effect of 0.94 Cohen’s d (equivalent to r = .42) seen across statistical tests reported in recent cognitive neuroscience, psychology and psychiatry articles from high impact journals {Szucs, 2017 #25}. However, we have not identified any studies which have adequate statistical precision to make strong inferences about the size or direction of the change in effect sizes over time. We know of just one study which attempted to examine the degree and direction of change in average effect sizes over time. {Paterson, 2015 #817} found a small negative correlation (r = -.05, 95% CI [`r transf.rtoz(-.05) - (1.96 * 1/sqrt(776 - 3))`, `r transf.rtoz(-.05) + (1.96 * 1/sqrt(776 - 3))`]) between the reported magnitude of correlations and their year of publication in 776 meta-analytic conclusions from meta-analyses in management psychology. The current analysis allows us to examine the average effect size reported in psychological research, and allows us to precisely estimate the change in effect sizes over time across fields of psychological research.

### Method

This analysis uses the dataset developed in {Nuijten, 2015 #550}, a study examining the number of errors in statistical tests reported in psychology articles (all data from https://osf.io/gdr4q/).  This dataset includes `r nrow(data)` statistical test results from `r length(unique(data$Source))` articles. Nuijten et al. (2018) extracted this database using regular expressions, exploiting the APA style guide's strict rules for reporting statistical test results and to extract all chi square, t tests, F tests, Z tests, and correlations reported in APA style from articles published in eight major psychology journals from 1985 to 2013.

The current paper only uses a subset of these studies, those reporting in journals for which results were available going back to 1985. This analysis therefore includes data from five psychology journals chosen to be representative of the main subdisciplines of psychology research; Journal of Applied Psychology (JAP; Applied Psychology), Journal of Consulting and Clinical Psychology (JCCP; Clinical Psychology) , Developmental Psychology (DP; Developmental Psychology), Journal of Experimental Psychology: General (JEPG; Experimental Psychology), and Journal of Personality and Social Psychology (JPSP; Social Psychology). The included subset is made up of a total of `r nrow(dat)` statistical test results from a total of `r length(unique(dat$article))` articles published from 1985 to 2013.

### Effect size extraction and conversion 

All test statistics were converted to Fisher Z transformed correlation coefficients (henceforth $Fisher_z$) following {Open Science Collaboration, 2015 #611} for visualization and analysis, see Supplementary Materials 1 for detailed explanations of the  transformations used. Negative effect sizes (i.e., negative correlations) were set to be positive for analysis and visualization. Standard errors were estimated as $\sqrt{1/(n - 3)}$, taking the degrees of freedom for the denominator plus two as n (or just the degrees of freedom plus two in the case of correlations and t tests). Because typical APA notation for z tests does not report the included sample size in a standardized format (and therefore this information was not available in this dataset), Z scores were excluded from analyses (n = `r sum(dat$Statistic=="Z")`). As it was not possible to derive valid standard errors for F statistics with denominator degrees of freedom above 1 (n = `r sum(dat$Statistic=="F" & dat$df1 > 2, na.rm = T) `) or for Chi square statistics (n = `r sum(dat$Statistic=="Chi2")`), these analyses were excluded from the multilevel meta-analysis (see Figure 1 for histograms comparing effect sizes included in each analysis and those derived from the entire sample). An additional subset of results were excluded from all analyses as they produced standard errors or Z transformed correlation coefficients which could not be estimated or were infinite (n = `r sum((dat$z == Inf | is.na(dat$z) | dat$df2 < 6 ) & (dat$Statistic!="Chi2" & (!(dat$Statistic=="F" & dat$df1 > 1)) & (dat$Statistic != "Z")) )`, e.g., studies which reported impossible test statistics such as "F(0, 55) = 5.71, p < .05" or "r(66) = 5.42, p < .001" and studies with denominator degrees of freedom below 6). This left a total of `r nrow(datMeta)` effect size estimates with valid standard errors from `r length(unique(datMeta$article))` articles were extracted and are included in the meta-regression analyses below.


## Analysis 

Multilevel meta-regression was performed to examine the relationship between year of publication and reported effect sizes.

$ES_j = \gamma_0 + \gamma_1Year_j + u_{effect} + u_{article} + u_{journal} + e_{j}$

The multilevel-meta-regression includes random effects for individual tests ($u_{effect}$), articles ($u_{article}$) and journals ($u_{journal}$), and includes year of publication of each article as a fixed effect ($\gamma_1Year$). This analysis was performed using nlme {Pinheiro, 2018 #1027} in R version 3.5.1 {R Development Core Team, 2018 #314} using restricted maximum likelihood estimation. 

As a check on whether the exclusion of F statistics with $df_1$ of greater than one and $\chi^2$ analyses is likely to change the results, this multilevel model was reperformed including all of the statistical tests for which correlations could be estimated and estimating standard errors as $\sqrt{1/(df_2 - 5)}$ for F tests and $\sqrt{1/(df - 5)}$ for $\chi^2$ tests. This analysis includes a total of n = `r nrow(datComplete)` effects after excluding all invalid results (i.e., analyses where it was not possible to estimate a standard error or effect size using the above methods due to issues such as impossible test statistic values that were likely typos). This analysis led to estimates of the change in effect sizes over time which are practically identical to those presented below (i.e., which differed by < `r 0.002 # calculated below see "differenceAllMeta"`). 

Robustness checks were performed to assess the robustness of these results to model specification changes. Including random slopes by journal, fixed effects for test statistic type, allowing the change over time to vary by test statistic type, or not accounting for the variance of each effect size estimates did not lead to any substantial difference in the results of this analysis. See Supplementary Materials 2 for a more in-depth discussion of each of these models.  

#### Exploratory analyses Accounting for 'peripheral' tests

Due to the large sample size included in this analysis and the extremely small expected effects (and hence, large sample sizes required), it was not feasible to manually label which statistical tests included were, for example, manipulation tests or randomization tests, and some of the tests reported and included in the current analysis are certainly not tests of the substantive hypotheses of each included paper. This means that any observed change in effect sizes could be driven by changes in reporting practices, such as a changing number of reported manipulation or randomization checks over time. 

In order to account for this issue, I performed two additional exploratory analyses to examine whether the trends seen across all reported statistical tests are also seen in subsets of results that may be more likely to represent the "main" effects under study. In order to do so multilevel mixed effects meta-regression was performed looking just at (a) the first statistical test reported in each paper and (b) the largest effect size reported in each paper. For (b), in 84 cases where there were ties within papers for the largest effect size, the first of the two equal outcome sizes was used.

$ES_j = \gamma_0 + \gamma_1Year_j + u_{article} + u_{journal} + e_{j}$

These multilevel-meta-regressions included random effects for individual articles ($u_{article}$) and journal ($u_{journal}$), and included year of publication of each article as a fixed effect ($\gamma_1Year$). These analyses did not include random effects for each statistical test, as each article only provides a single effect size. Valid standard errors were calculable for effect sizes from a total of `r nrow(datMetaLargest)` articles for analyses (a) and (b), all of which were included in these analyses. These analyses were performed using the R package metafor {Viechtbauer, 2010 #796} using restricted maximum likelihood estimation.

### Deviations from preregistration

All analyses were tested and developed on a randomly selected subset of 0.01% of the dataset before being preregistered. Thirty-six reported test statistics were excluded using non-preregistered rules (3 which reported an r of 1, leading to an infinite Cohen's z, and 33 test which were parsed by statcheck as reporting impossible test statistics). All random effects meta-regressions were performed with an additional random effect at the lowest level (i.e., at the effect or article level) than was preregistered, as this was thought to be more conceptually appropriate as all tests within a paper cannot be assumed to have estimated the same parameter. The analyses performed on all data (i.e., those which do not attempt exclude peripheral tests) were estimated using the R package nlme in lieu of metafor {Viechtbauer, 2010 #796} as the memory requirements of metafor exceeded those that we had access to (requiring > 160 gbs of RAM). The results should be identical for all practical purposes (e.g., reperforming the two analyses which looked at just the first reported effect or the largest reported effect in nlme lead to parameter estimates that differed by less than .000001 from that produced using metafor, and the standard errors of each estimated parameter differed by less than `r max(abs(c(0.0870892894  - 0.0873152, 0.0006548126 - 0.0006565, 0.0371372978 - 0.0421990 , 0.0004857659 - 0.0005162))) # these are the standard errors for the intercept and the year coefficient from the model estimated with metafor (first) vs nlme (second), for the largest effects (first 2 comparisions) and first reported effects in each paper (comparisons 3 and 4)`). Any statistical tests that were not preregistered are identified as such below.

### Limitations

Before turning to the results of this analysis, it should be noted that the sample is limited to articles published in 5 APA journals in a limited time period (1985 - 2013) and the results may not generalize outside of this sample. It is possible for example that publication patterns have changed and studies with larger effect sizes tend to target other publications (or the reverse). Additionally, the method used to collect effect sizes from the literature, using regular expressions to extract statistical tests has its limitations. This method only captures statistical tests results reported in-text (i.e., not in tables) in APA style, and the observed effects could be driven by changes in how likely people are to report statistical tests in APA style (e.g., if people have become more likely to report small or non-significant results in text in full APA style over time).

## Results
### Descriptives

```{r correlation collapsing withing article}
averageESArticle <- datMeta %>% 
  group_by(article, year) %>%
  summarise(mean(z))

aggregatedCorzy <- cor.test(averageESArticle$`mean(z)`, averageESArticle$year)
```


```{r histograms}

All_data <- ggplot(data = datComplete, aes(x = r)) + geom_histogram(bins = 30, colour = "black", fill = "white") + theme_classic() + ylab("Count") + ggtitle("All effects")+ theme(plot.title = element_text(hjust = .5)) + xlab(" ") + ylim(c(0,16000)) 

Meta_anaylsis_data <- ggplot(data = datMeta, aes(x = r)) + geom_histogram(bins = 30, colour = "black", fill = "white") + theme_classic() + xlab(" ")  + ylab(NULL) + ggtitle("Meta-analytic subset")+ theme(plot.title = element_text(hjust = .5))+ ylim(c(0,12500)) 

  Largest_reported_effect <- ggplot(data = datMetaLargest, aes(x = transf.ztor(z))) + geom_histogram(bins = 30, colour = "black", fill = "white") + theme_classic() + xlab("Correlation coefficent") + ylab("Count") + ggtitle("Largest reported effect") + theme(plot.title = element_text(hjust = .5))+ ylim(c(0,750)) 

First_reported_effect <- ggplot(data = datMetaFirst, aes(x = transf.ztor(z))) + geom_histogram(bins = 30, colour = "black", fill = "white") + theme_classic() + xlab("Correlation coefficent") + ylab(NULL) + ggtitle("First reported effect") + theme(plot.title = element_text(hjust = .5))+ theme(plot.title = element_text(hjust = .5)) + ylim(c(0,750))

histograms <- plot_grid(All_data,Meta_anaylsis_data, Largest_reported_effect, First_reported_effect)

```

```{r histograms of the df}

All_data <- ggplot(data =
                     filter(
                     datComplete, 
                     df2 < 500
                     ), aes(x = df2)) + geom_histogram(bins = 30, colour = "black", fill = "white") + theme_classic() + ylab("Count") + ggtitle("All effects")+ theme(plot.title = element_text(hjust = .5)) + xlab(" ") # + ylim(c(0,16000)) 

Meta_anaylsis_data <- ggplot(data = 
                               filter(
                               datMeta, 
                     df2 < 500
                     )
                               , aes(x = df2)) + geom_histogram(bins = 30, colour = "black", fill = "white") + theme_classic() + xlab(" ")  + ylab(NULL) + ggtitle("Meta-analytic subset")+ theme(plot.title = element_text(hjust = .5)) # + ylim(c(0,12500)) 

  Largest_reported_effect <- ggplot(data = 
                               filter(
                               datMetaLargest, 
                     df2 < 500
                     ), aes(x = df2)) + geom_histogram(bins = 30, colour = "black", fill = "white") + theme_classic() + xlab("Degrees of freedom") + ylab("Count") + ggtitle("Largest reported effect") + theme(plot.title = element_text(hjust = .5)) 

First_reported_effect <- ggplot(data =
                               filter(
                                datMetaFirst, 
                     df2 < 500
                     ), aes(x = df2)) + geom_histogram(bins = 30, colour = "black", fill = "white") + theme_classic() + xlab("Degrees of freedom") + ylab(NULL) + ggtitle("First reported effect") + theme(plot.title = element_text(hjust = .5))+ theme(plot.title = element_text(hjust = .5)) 

histogramsDF2 <- plot_grid(All_data,Meta_anaylsis_data, Largest_reported_effect, First_reported_effect)

```

The mean effect size reported across the sample in correlation coefficient terms is `r mean(transf.ztor(datComplete$z))` and the median is `r median(transf.ztor(datComplete$z))`. Overall, the distribution of effect sizes in the meta-analytic subset is close to that seen in the full dataset, although the mean and median are slightly higher (at `r mean(datMeta$r)` and `r median(datMeta$r)` respectively). The effect sizes seen when examining only the highest effect size reported in each paper are much higher on average (with a mean correlation coefficient of `r mean(datMetaLargest$r)` and a median of  `r median(datMetaLargest$r)`). See Table 1 and Figure 1 for a full list of descriptives about, and histograms of, the distribution of effect sizes in each subsample, and Figure 2 for a QQ plot of effect sizes from the Meta-analytic subset against the effects seen in the full sample. It is noteworthy that the median effect size seen across the whole sample (r = `r median(datComplete$r)`) is remarkably close to Cohen’s suggested “medium” effect size benchmark value of r = .3, although the upper are the lower and upper quantiles (`r quantile(datComplete$r, c(.25, .75))`) are, respectively, higher and lower than Cohen's "Small" and "Large" effect size benchmarks (.1 and .5), an occurrence that has been noted in other subfields of research {Quintana, 2017 #836}.

```{r}
histograms
```

Figure 1. Histograms of reported effect sizes transformed to correlation coefficients, for all results which could be transformed to correlation coefficients, the meta-analytic subset, the  the first reported effect size in each paper, and the largest reported effect size in each paper.

```{r, fig.height=5, fig.width=5}
qqplot(dat$z[!is.na(dat$z)], datMeta$z, xlab = "Full dataset", ylab = "Meta-analytic subset")
abline(a = 0, b = 1)
```

Figure 2. A qq plot of effect sizes in $Fisher_z$ scores from all data plotted against the meta-analytic subset.

```{r table_demo_df}
# Calculating descriptives 
descriptivesTab <- psych::describe(datComplete[,c("r", "z")], skew = T, quant = c(.25, .75))
descriptivesTab[c(3,4),] <- psych::describe(datMeta[,c("r", "z")], skew = T, quant = c(.25, .75))
descriptivesTab[c(5,6),] <- psych::describe(datMetaLargest[,c("r", "z")], skew = T, quant = c(.25, .75))
descriptivesTab[c(7,8),] <- psych::describe(datMetaFirst[,c("r", "z")], skew = T, quant = c(.25, .75))
names(descriptivesTab)[1] <- "Subsample"
descriptivesTab <- as.tibble(descriptivesTab)
descriptivesTab$Subsample <- c("All data", "All data", "Meta-analytic subset", "Meta-analytic subset", "Largest reported effect","Largest reported effect", "First reported effect", "First reported effect")
descriptivesTab$`Effect size` <- rep(c("Correlation","Fisher z"), 4)

descriptivesTab <- descriptivesTab[,c(1,16,2,3,4,8,14,5,15,9,10,11,12)]
names(descriptivesTab) <- c("Subsample", "Effect size", "n", "Mean", "sd", "Min","25th percentile", "Median", "75th percentile", "Max", "Range", "Skew", "Kurtosis")

```

The mean degrees of freedom for the denominator (or just degrees of freedom in the case of Chi Square, t, F or r tests) reported across the sample is `r mean(datComplete$df2)` and the median is `r median(datComplete$df2)`. Again, the distribution of degrees of freedom in the meta-analytic subset is close to that seen in the full dataset, although the mean and median are slightly lower (at `r mean(datMeta$df2)` and `r median(datMeta$df2)` respectively). The distribution of degrees of freedom seen when examining only the highest effect size reported in each paper is slightly more positively skewed that that seen in the other subsets (with a mean value of `r mean(datMetaLargest$df2)` and a median of  `r median(datMetaLargest$df2)`). The mean and median degrees of freedom seen when examining only the first statistical test reported in each paper are slightly higher that that seen in the other subsets  (at `r mean(datMetaFirst$df2)` and `r median(datMetaFirst$df2)` respectively). See Table 2 and Figure 2 for a full list of descriptives about, and histograms of, the distribution of degrees of freedom in each subsample.

```{r}
histogramsDF2
```

Figure 2. Histograms of reported degrees of freedom for the denominator (or just degrees of freedom in the case of Chi Square, t, F or r tests), for all data, the meta-analytic subset, the first reported effect size in each paper, and the largest reported effect size in each paper.


```{r table_demo}
# Calculating descriptives 
descriptivesTab <- psych::describe(datComplete$df2, skew = T, quant = c(.25, .75))
descriptivesTab[c(2),] <- psych::describe(datMeta$df2, skew = T, quant = c(.25, .75))
descriptivesTab[c(3),] <- psych::describe(datMetaLargest$df2, skew = T, quant = c(.25, .75))
descriptivesTab[c(4),] <- psych::describe(datMetaFirst$df2, skew = T, quant = c(.25, .75))
 names(descriptivesTab)[1] <- "Subsample"
descriptivesTab <- as.tibble(descriptivesTab)
descriptivesTab$Subsample <- c("All data", "Meta-analytic subset",  "Largest reported effect", "First reported effect")

descriptivesTab <- descriptivesTab[,c(1,2,3,4,8,14,5,15,9,10,11,12)]
names(descriptivesTab) <- c("Subsample", "n", "Mean", "sd", "Min","25th percentile", "Median", "75th percentile", "Max", "Range", "Skew", "Kurtosis")

```


Table 2. Descriptives of reported degrees of freedom for the denominator (or just degrees of freedom in the case of Chi Square, t, F or r tests), for all data, the meta-analytic subset, the first reported effect size in each paper, and the largest reported effect size in each paper.

`r kable(descriptivesTab)`

```{r plots2, fig.width = 10, fig.height=4.5}
overallMean <- datComplete %>% 
  #group_by(year, article) %>%
  #summarise(z = mean(z))%>% 
  group_by(year) %>%
  summarise("Mean ES" = mean(z, na.rm = T))

overallMedian <- datComplete %>% 
  #group_by(year, article) %>%
  #summarise(z = mean(z))%>% 
  group_by(year) %>%
  summarise("Median ES" = median(z, na.rm = T), "SD ES" = sd(z))

metaMean <- datMeta %>% 
  #group_by(year, article) %>%
  #summarise(z = mean(z))%>% 
  group_by(year) %>%
  summarise("Mean ES (meta-analysis subset)" = mean(z, na.rm = T))

metaMedian <- datMeta %>% 
  #group_by(year, article) %>%
  #summarise(z = mean(z))%>% 
  group_by(year) %>%
  summarise("Median ES (meta-analysis subset)" = median(z, na.rm = T), "SD meta ES" = sd(z))

largestMean <- datMetaLargest %>% 
  #group_by(year, article) %>%
  #summarise(z = mean(z))%>% 
  group_by(year) %>%
  summarise("Mean largest ES" = mean(z))

largestMedian <- datMetaLargest %>% 
  #group_by(year, article) %>%
  #summarise(z = mean(z))%>% 
  group_by(year) %>%
  summarise("Median largest ES" = median(z), "SD largest ES" = sd(z))

firstMean <- datMetaFirst %>% 
  #group_by(year, article) %>%
  #summarise(z = mean(z))%>% 
  group_by(year) %>%
  summarise("Mean first ES" = mean(z))

firstMedian <- datMetaFirst %>% 
  #group_by(year, article) %>%
  #summarise(z = mean(z))%>% 
  group_by(year) %>%
  summarise("Median first ES" = median(z), "SD first ES" = sd(z))

# Summary table
summaryTab <- data.frame(overallMean,
overallMedian[,c(2,3)],
metaMean[,2],
metaMedian[,c(2,3)],
largestMean[,2],
largestMedian[,c(2,3)],
firstMean[,2],
firstMedian[,c(2,3)])

# Summary table
summaryPlotTab <- data.frame(overallMean,
overallMedian[,2],
metaMean[,2],
metaMedian[,2],
largestMean[,2],
largestMedian[,2],
firstMean[,2],
firstMedian[,2])

summaryPlot <- gather(summaryPlotTab, key = "Summary type", value = "Average ES", -year)

# Reordering factor levels 
summaryPlot$`Summary type` <- factor(summaryPlot$`Summary type`, levels = names(colMeans(summaryPlotTab[,-1]))[order(decreasing = T, colMeans(summaryPlotTab[,-1]))])

sumMeans <- ggplot(summaryPlot[str_detect(summaryPlot$`Summary type`, "Mean"),], aes(y = `Average ES`, x = year, group = `Summary type`, colour = `Summary type` )) + 
  geom_line() + geom_point() + ylim(0,1) + theme_classic() + ylab("Average effect size (Fisher z)") + xlab("Year") + scale_color_discrete(name = "Summary type", labels = str_replace_all(levels(summaryPlot$`Summary type`), "\\.", " ")) + theme(legend.position="none") + ggtitle("Mean") + theme(plot.title = element_text(hjust = .5))

sumMedians <- ggplot(summaryPlot[str_detect(summaryPlot$`Summary type`, "Median"),], aes(y = `Average ES`, x = year, group = `Summary type`, colour = `Summary type` )) + 
  geom_line() + geom_point() + ylim(0,1) + theme_classic() + ylab(NULL) + xlab("Year") + scale_color_discrete(name = "Subsample", labels = c("Largest ES", "First ES", "Meta analysis subset", "All data")) + ggtitle("Median")+ theme(plot.title = element_text(hjust = .5))

plot_grid(sumMeans, sumMedians, rel_widths =  c(1, 1.5))

```

Figure [ESs over time]. Plots of the mean and median effect sizes per year (in Fisher Z transformed correlation coefficients) by the subsamples used in analyses.

```{r exmaining effect sizes by statistic, fig.width = 10, fig.height=4.5}
trendsByType <- datMeta %>% 
  group_by(Statistic, year) %>%
  summarise("Mean ES" = mean(z), "SD first ES" = sd(z))

trendsByTypeLargest <- datMetaLargest %>% 
  group_by(year) %>%
  summarise("Mean ES" = mean(z), "SD first ES" = sd(z))

averageEffect <- ggplot(trendsByType, aes(x = year, y = `Mean ES`, group = Statistic, fill = Statistic, colour = Statistic)) + geom_line() + ylim(0,1) + geom_point() + ylab("Mean effect size (Fisher z)") + xlab("Year") + theme(legend.position="none")

trendInType <- datMeta %>%
  group_by(Statistic, year) %>% 
  summarise(n = n())

nPerYear <- datMeta %>%
  group_by(year) %>% 
  summarise(perYear = n())

trendInType <- left_join(trendInType, nPerYear, by = "year")

trendInType$prop <- trendInType$n / trendInType$perYear

propTest <- ggplot(trendInType, aes(x = year, y = prop, group = Statistic, fill = Statistic, colour = Statistic)) + geom_line() + geom_point() + xlab("Year") + ylab("Proporiton of reported statistical tests") + ylim(0,1)

plot_grid(averageEffect,propTest, rel_widths = c(1, 1.25))
```

Figure [effectByStat]. A plot of the mean effect sizes per year (left, in Fisher Z transformed correlation coefficients) as transformed from the various effect size measures, and of the proportion of reported statistical tests of each type included in the current analysis (right).

```{r fig.width = 10, fig.height=4.5}

nPerArticle <- datMeta %>%
  group_by(article, journal, year) %>% 
  summarise(n = n()) %>%
  group_by(year, journal) %>% 
  summarise(MeanN = mean(n), n = n())

nPerArticleOverall <- datMeta %>%
  group_by(article, journal, year) %>% 
  summarise(n = n()) %>%
  group_by(year) %>% 
  summarise(MeanN = mean(n), n = n())
nPerArticleOverall$journal <- "All articles"

nPerArticlePre1991 <- datMeta %>%
  group_by(article, journal, year) %>% 
  summarise(n = n()) %>%
  group_by(year < 1991) %>% 
  summarise(MeanN = mean(n), n = n())

nPerArticlePost2008 <- datMeta %>%
  group_by(article, journal, year) %>% 
  summarise(n = n()) %>%
  group_by(year > 2008) %>% 
  summarise(MeanN = mean(n), n = n())

nPerArticle <- bind_rows(nPerArticle, nPerArticleOverall)

nTests <- ggplot(nPerArticle, aes(x = year, y = MeanN, group = journal, colour  = journal)) + geom_line() + geom_point() + xlab("Year") + ylab("Mean number of statistical tests per article") + ylim(0,round(max(nPerArticle$MeanN)+1)) + theme(legend.position="none")

nArticles <- ggplot(nPerArticle, aes(x = year, y = n, group = journal, colour  = journal)) + geom_line() + geom_point() + xlab("Year") + ylab("Number of articles") + ylim(0,round(max(nPerArticle$n)+1)) +scale_color_discrete(name = "Journal")

plot_grid(nTests, nArticles, rel_widths = c(1, 1.4))
```

Figure [n tests]. A plot of the mean number of tests reported in each article (left), and the number of articles reported by journal and overall (right).

```{r trends by journal}
trendsByJournal <- datComplete %>% 
  group_by(journal, year) %>%
  summarise("Mean ES" = mean(z, na.rm = T), "SD first ES" = sd(z))

journalMeanES <- datComplete %>% 
  group_by(journal) %>%
  summarise("Mean ES" = mean(z, na.rm = T), "SD first ES" = sd(z))

trendsByJournal$Journal <- factor(trendsByJournal$journal, levels = journalMeanES$journal[order(journalMeanES$`Mean ES`, decreasing = T)])

ggplot(trendsByJournal, aes(x = year, y = `Mean ES`, group = Journal, fill = Journal, colour = Journal)) + geom_line() + ylim(0,1) + geom_point() + ylab("Mean Effect size (Fisher z)") + xlab("Year") + theme_classic()

```

Figure [trends by J]. A plot of the mean effect size reported in each journal by Year.  

The number of t, F and correlational statistical tests reported in APA style per article has increased considerably over time in this sample, from a mean of `r nPerArticlePre1991[2,2]` reported per article in 1985 - 1990, to a mean of `r nPerArticlePost2008[2,2]` from 2009 - 2013 (note that these averages only include articles with at least one reported statistical test). See figure [n tests] for a plot of the mean number of tests reported per article over time, and a plot of the number of articles reported in each journal included in this current analysis.

The proportion of t tests in this sample has increased over time, relative to F tests and correlations, see figure [effectByStat]. The average reported effect size from each type of statistical test is similar (mean $Fisher_z$ for correlations = `r mean(filter(datMeta, datMeta$Statistic == "r")$z)` , mean for F statistics = `r mean(filter(datMeta, datMeta$Statistic == "F")$z)`, mean for t statistics = `r mean(filter(datMeta, datMeta$Statistic == "t")$z)`), and the trend over time appears to be consistent among all sources of effect sizes (see figure [effectByStat]). An additional, non-preregistered analysis supports the finding that the trend in effect sizes over time is virtually identical across types of statistical tests, including interaction terms between year and test statistic type for each type of effect size led to negligible estimated interaction effects of .0005 or less (see Supplementary materials 2 for full model output).


### Results

```{r data analaysis, echo = FALSE}
# Running the meta analysis, commented out for speed. 
#mod <- lme(z ~ 1 + I(year - mean(year)), random = ~ 1 |  journal / article / id,  weights = varFixed(~SEz^2), data = datMeta, control = lmeControl(sigma = 1, apVar = T))
#write_rds(mod, "data/mod.RDS")
mod <- read_rds("data/mod.RDS")

# just making pretty output 
niceModSum <- data_frame(" " = c("Intercept", "Year",rep(NA,4)), 
  Estimate = c(mod$coefficients$fixed, rep(NA,4)), 
           "95% CI LB" = c(intervals(mod)$fixed[,1],rep(NA, 4)),
           "95% CI UB" = c(intervals(mod)$fixed[,3],rep(NA, 4)),
           SE = c(diag(sqrt(summary(mod)$varFix)), rep(NA, 4)),
           "p" = c("< .001", "< .001", rep(NA, 4)),
           "Random effects" = c(rep(NA, 2), 
                                paste0("Effect variance = ", round(as.numeric(VarCorr(mod))[6], 3), ", n = ", nrow(datMeta)),
                                paste0("Article variance = ", round(as.numeric(VarCorr(mod))[4], 3), ", n = ",length(unique(datMeta$article))),
                                paste0("Journal variance = ", round(as.numeric(VarCorr(mod))[2], 3), ", n = ", length(unique(datMeta$journal))),
                                paste0("QE(", nrow(datMeta) - 1, ") = ", round(sum(residuals(mod, type="pearson")^2),2), 
                                       ", p = < .001") 
                                ))

####   I^2 for ML metas =  following Nakagawa, 2012 #1023}
# W <- diag(1/datMeta$SEz^2)
# X <- model.matrix(z ~ 1 + I(year -  mean(year)) , data = datMeta)
# P <- W - W %*% X %*% solve(t(X) %*% W %*% X) %*% t(X) %*% W
# Io2 <- 100 * sum(0.231 + 0.182 + 0.0745) / (sum(REMod$sigma2) + (nrow(datMeta)-2)/sum(diag(P)))

# Testing without REs on lowest level 
# modSimp <- lme(z ~ 1 + I(year - mean(year)), random = ~ 1 |  journal / article,  weights = varFixed(~SEz^2), data = datMeta, control = lmeControl(sigma = 1, apVar = T))
# calculating simplest effect 
# diffsimp <- mod$coefficients$fixed - modSimp$coefficients$fixed

# All data analysis - i.e., using approx standard errors
# modAll <- rma.mv(z, V = SEz^2, random = ~  1 |  journal / article / id, mods = (year-mean(year)), data = datComplete)
# modAll  <- lme(z ~ 1 + I(year - mean(year)), random = ~ 1 |  journal / article / id,  weights = varFixed(~SEz^2), data = # datComplete, control = lmeControl(sigma = 1, apVar = T))
# write_rds(modAll, path = "data/modAll.RDS")

modAll <- read_rds(path = "data/modAll.RDS")
# modAll <- read_rds("data/modAll.RDS")
# calculating difference between models
differenceAllMeta <- mod$coefficients$fixed[2] - modAll$coefficients$fixed[2]
 # ranef(mod)
```

A non-preregistered analysis shows that there is a low correlation between year of publication and effect size ($Fisher_z$) of r(`r length(datMeta$z)-2`) =`r corzy <- cor.test(datMeta$z, datMeta$year); corzy[4]`, p `r ifelse(round(corzy[[3]],3)==0, "< .001", round(corzy[[3]],3))`, 95% CI [`r corzy$conf.int[c(1,2)]`]. When averaging the effect sizes seen in each article to avoid issues of non-independence of statistical tests within articles and estimating the correlation between year and effect size we find a small association between effect size and year of publication, r(`r nrow(averageESArticle)-2`) = `r aggregatedCorzy[[4]]`, p `r ifelse(round(aggregatedCorzy[[3]],3) == 0, "< .001", round(aggregatedCorzy[[3]],3))`, 95% CI [`r aggregatedCorzy$conf.int[c(1,2)]`].

```{r excluding all but largest effect in each paper and the first effect in each paper}

# Commented out because it takes ~ 40 minutes on a fast computer to run model. 
#modLargest <- rma.mv(z, V = SEz^2, random = ~ 1 | journal / factor(article), mods = (year - mean(year)), data = datMetaLargest)
# write_rds(modLargest, "data/ModLargest.RDS")
modLargest <- read_rds("data/ModLargest.RDS")


# ranef(modLargest)
#modFirst <- rma.mv(z, V = SEz^2, random = ~ 1 | journal / factor(article), mods = (year - mean(year)), data = datMetaFirst)
# write_rds(modFirst, "data/modFirst.RDS")
modFirst <- read_rds("data/modFirst.RDS")

# ranef(modFirst)
```


```{r predicting scores from random effects models}
# Main difference between 1985 and today 
changeOverTime <-  transf.ztor(mod$coefficients$fixed[1] + (mod$coefficients$fixed[2] * (2013 - mean(datMeta$year)))) - transf.ztor(mod$coefficients$fixed[1] + (mod$coefficients$fixed[2] * (1985 - mean(datMeta$year))))

# largest effect difference between 1985 and today
changeOverTimeLargest <- transf.ztor(predict(modLargest, (2013 - mean(datMetaLargest$year)))[[1]])- transf.ztor(predict(modLargest, (1985 - mean(datMetaLargest$year)))[[1]]) 

# first effect difference betweeen 1985 and today 
changeOverTimeFirst <- transf.ztor(predict(modFirst, (2013 - mean(datMetaFirst$year)))[[1]]) - transf.ztor(predict(modFirst, (1985 - mean(datMetaFirst$year)))[[1]])

#  I^2 for ML metas =  following Nakagawa, 2012 #1023}
Io2 <- 100 - ((nrow(datMeta)-1) / sum(residuals(mod, type="pearson")^2))

#  I^2 for ML metas =  following Nakagawa, 2012 #1023}
# W <- diag(1/datMetaLargest$SEz^2)
# X <- model.matrix(modLargest)
# P <- W - W %*% X %*% solve(t(X) %*% W %*% X) %*% t(X) %*% W
# Io2Largest <- 100 * sum(modLargest$sigma2) / (sum(modLargest$sigma2) + (modLargest$k-modLargest$p)/sum(diag(P)))
# write_rds(Io2Largest, "data/Io2Largest.RDS")
Io2Largest <- read_rds("data/Io2Largest.RDS")

# I^2 for ML metas =  following Nakagawa, 2012 #1023} Commented out for speed
#   W <- diag(1/datMetaFirst$SEz^2)
#   X <- model.matrix(modFirst)
#   P <- W - W %*% X %*% solve(t(X) %*% W %*% X) %*% t(X) %*% W
#   Io2First <- 100 * sum(modFirst$sigma2) / (sum(modFirst$sigma2) + (modFirst$k-modFirst$p)/sum(diag(P)))
# write_rds(Io2First, "data/Io2First.RDS")
Io2First <- read_rds("data/Io2First.RDS")

```

The multilevel meta-regression including all data with valid standard errors shows an estimated $Fisher_z$ decrease per year of `r mod$coefficients$fixed[2]`, 95% CI [`r intervals(mod)$fixed[2,1]`, `r intervals(mod)$fixed[2,3]`]. Unsurprisingly given that articles likely report multiple statistical tests of different hypotheses in each article there is a large amount of unexplained heterogeneity in effect sizes, `r niceModSum[6,7]`, $I^2$ = `r round(Io2)`% {Nakagawa, 2012 #1023}. This suggests that `r round(Io2)`% of residual variance in effect sizes is due to effect size heterogeneity (i.e., variance in the true effect size differences), while the remaining `r 100-round(Io2)`%  is attributable to sampling variance. More variance is attributable to the article and effect level than to the journal (with estimated standard deviations for each random effect of $\sigma_{article}$ = `r round(as.numeric(intervals(mod)$reStruct$article[2]), 3)`, 95% CI [`r intervals(mod)$reStruct$article[c(1,3)]`], 
$\sigma_{effect}$ = `r round(as.numeric(intervals(mod)$reStruct$id[2]), 3)`, 
95% CI [`r intervals(mod)$reStruct$id[c(1,3)]`], 
compared to $\sigma_{journal}$ = `r round(as.numeric(intervals(mod)$reStruct$journal[2]), 3)`, 95% CI [`r intervals(mod)$reStruct$journal[c(1,3)]`]), representing a very low interclass correlation (ICC) for the journal of  `r round(as.numeric(VarCorr(mod)[c(2)]) / sum(as.numeric(VarCorr(mod)[c(2,4,6)])), 3)`, with most variance in true effect size differences being accounted for at the article and effect level (with an ICC for the article of `r round(as.numeric(VarCorr(mod)[c(4)]) / sum(as.numeric(VarCorr(mod)[c(2,4,6)])), 3)` and an ICC for the effect of `r round(as.numeric(VarCorr(mod)[c(6)]) / sum(as.numeric(VarCorr(mod)[c(2,4,6)])), 3)`).

Table [nice MLME sum datMeta]. Multilevel meta-regression output including all applicable effects. $n_{effects}$ = `r nrow(datMeta)`, $n_{articles}$ = `r length(unique(datMetaFirst$article))`.
`r kable(niceModSum)`

#### Exploratory analyses

Including only the first reported statistical test in each paper provides similar results, suggesting a small decrease over time, with a `r modFirst$b[2]` (95% CI [`r modFirst$ci.lb[2]`, `r modFirst$ci.ub[2]`]) $Fisher_z$ estimated yearly change according to the model including only the first reported effect effects. The estimated $I^2$ value (`r round(Io2First,2)`%) is substantively identical to that of the model including all applicable effects. Looking at the variance partitioning, more variance is attributable to the article level than to the journal level ($\sigma_{article}$ = `r round(sqrt(modFirst$sigma2[2]), 3)`, 95% CI [`r `], compared to $\sigma^2_{journal}$ = `r round(sqrt(modFirst$sigma2[1]), 3)`, 95% CI [`r `]), representing a ICC for the journal of `r round(modFirst$sigma2[1] / sum(modFirst$sigma2), 3)`, with most variance accounted for at the article level (ICC for the article `r round(modFirst$sigma2[2] / sum(modFirst$sigma2), 3)`).

Table [nice MLME sum datMetaFirst]. Multilevel meta-regression output including the first reported effect size in each article, $n_{effects}$ = `r nrow(datMetaFirst)`.
`r kable(niceMLMESum2(modFirst))`

Including only the largest effect reported in APA style in each paper leads to a different conclusion, a predicted yearly $Fisher_z$ increase of `r  options(digits=4); modLargest$b[2]` (95% CI [`r modLargest$ci.lb[2]`, `r modLargest$ci.ub[2]`]). `r options(digits=3)` Again, the estimated $I^2$ value (`r round(Io2Largest,2)`) is functionally identical to those of the dataset including all data, and more variance is again attributable to the article level than to the journal level ($\sigma_{article}$ = `r round(sqrt(modLargest$sigma2[2]), 3)`, compared to $\sigma_{journal}$ = `r round(sqrt(modLargest$sigma2[1]), 3)`), leading to a low interclass correlation (ICC) for the journal of `r round(modLargest$sigma2[1] / sum(modLargest$sigma2), 3)`, with most variance in accounted for at the article level (with an ICC for the article of `r round(modLargest$sigma2[2] / sum(modLargest$sigma2), 3)`).

Table [nice MLME sum datMetaLargest]. Multilevel meta-regression output including just the largest reported effect size in each article, $n_{effects}$ = `r nrow(datMetaLargest)`.
`r kable(niceMLMESum2(modLargest))`

### Discussion

Overall, there was a small decrease in the mean effect sizes seen across the examined time period; going from a mean reported effect of  r = `r mean(filter(datMeta, year < 1990)$r)` between 1985 - 1990, to a mean reported effect size of  `r mean(filter(datMeta, year > 2008)$r)` in last five years included in this dataset, 2009 - 2013. The results of the random effects meta-regression accounting for random effects nested within articles and journals shows an estimated yearly decrease in effect sizes of `r mod$coefficients$fixed[2]` (95% CI [`r intervals(mod)$fixed[2,1]`, `r intervals(mod)$fixed[2,3]`]) in $Fisher_z$ units. This corresponds to an estimated correlation coefficient decrease of `r changeOverTime`0 from 1985 to 2013. 

```{r}
median1990 <- (datComplete %>% filter(year < 1989) %>% summarise(median(r)))
median2008 <- datComplete %>% filter(year > 2008) %>% summarise(median(r))
n1990 <- pwr::pwr.r.test(r = median1990[[1]], power = .95)$n
n2008 <- pwr::pwr.r.test(r = median2008[[1]], power = .95)$n

```

The estimated decrease in average effect sizes over this time period is small, but could represent a meaningful change in many areas of research. As an example, a researcher planning an experiment and hoping to reach 95% power to detect the average effect size seen in from 2009-2013 (r = `r median2008[[1]]`), as compared to the average effect seen from 1985 to 1989 (r = `r median1990[[1]]`) would have to recruit an additional n `r round(n2008 - n1990)` participants (assuming a simple correlational design; {Champely, 2018 #1069}).

There are a number of possible explanations for this decrease. Firstly, it is possible that the large, obvious, effects in psychology have already been discovered, and that this means that psychologists are now turning to study smaller, more subtle effects. If this is the case, and effect sizes under study in the scientific literature have in fact decreased, in combination with the findings from chapter [meta], this suggests that the average statistical power of psychological research has decreased over time.

Another possible explanation for the decrease in effect sizes over time is that it could be driven by changes in reporting and analysis practices as opposed to changes in the magnitude of the effects under study. Although the proportion of papers that report at least one statistically significant result has barely changed over time (see Figure [at least one significant result]), there has been a slight decrease in the proportion of reported statistical tests that are statistically significant (see Figure [Significant results over time]). If reporting non-significant results has become more common over time, this could lead to a decrease in the average reported effect size as small results that otherwise might not have been represented in the literature may be available.

```{r}
nPerY <- datMeta %>%
  group_by(article, year) %>% 
  summarise(n = n()) %>%
  group_by(year) %>% 
  summarise(MeanN = mean(n))

ggplot(nPerY, aes(x = year, y = MeanN)) + geom_line() + geom_point() + xlab("Year") + ylab("Mean n tests per article") + ylim(0,round(max(nPerArticle$MeanN)+1)) + theme(legend.position="none")

# a correlation of `r corSigYear$estimate` between year of publication and the mean proportion of studies reporting year year, and a mean proportion of signifiacnt reuslts from 1985 to 1990 of `r mean(significantProportion$averageProp[1:5])` comapred to a mean proportion of sigificant results from 2009 to 2013 of `r mean(significantProportion$averageProp[c(nrow(significantProportion):(nrow(significantProportion)-5))])`. Meanwhile, 

```

Figure [Significant results over time]. A plot of the mean number of statistical tests reported per article.

```{r}
meanZPerY <- datMeta %>%
  group_by(article, year) %>% 
  summarise(n = mean(z)) %>%
  group_by(year) %>%
  summarise(meanZ = mean(n)) 

ggplot(meanZPerY, aes(x = year, y = meanZ)) + geom_line() + geom_point() + xlab("Year") + ylab("Fisher Z score") + ylim(0, 1) + theme(legend.position="none")

# a correlation of `r corSigYear$estimate` between year of publication and the mean proportion of studies reporting year year, and a mean proportion of signifiacnt reuslts from 1985 to 1990 of `r mean(significantProportion$averageProp[1:5])` comapred to a mean proportion of sigificant results from 2009 to 2013 of `r mean(significantProportion$averageProp[c(nrow(significantProportion):(nrow(significantProportion)-5))])`. Meanwhile, 

```
Figure [Fisher Z scores]. A plot of the mean Fisher Z score amalgamated at the article level (i.e., the mean Fisher Z score across the sample, averaging within each article to avoid weighting articles that report more test statistics more highly than articles that report fewer). 

```{r}
significantProportion <- datMeta %>%
  group_by(article, year) %>%
  summarise(prop = mean(Computed <= .05)) %>%
  ungroup() %>%
#  mutate(year = as.numeric(as.character(year))) %>%
    group_by(year) %>%
  summarise(averageProp = mean(prop))
  
qplot(significantProportion$year, significantProportion$averageProp, 
      xlab = "Year", 
      ylab = "Mean proportion of tests significant", 
      ylim = c(0,1), xlim = c(1985, 2015))+ geom_line()

corSigYear <- cor.test(significantProportion$year,
                       significantProportion$averageProp) 

```

Figure [Significant results over time]. A plot of the mean proportion of reported statistical tests that are significant over time. The proportion of significant tests was calculated at the article level and then averaged for display here.

```{r}
proportionAtLeastOne <- datMeta %>%
  group_by(article, year) %>%
  summarise(prop = mean(Computed <= .05)) %>%
  ungroup() %>%
#  mutate(year = as.numeric(as.character(year))) %>%
    group_by(year) %>%
  summarise(averageProp = mean(prop > 0))
  
qplot(proportionAtLeastOne$year, proportionAtLeastOne$averageProp, xlab = "Year", ylab = "Prop. articles with >1 significant result", ylim = c(0,1), xlim = c(1985, 2015)) + geom_line()

corSigAtLeast1  <- cor.test(proportionAtLeastOne$year,
                            proportionAtLeastOne$averageProp)

```

Figure [Significant results over time]. A plot of the number of papers that report at least one significant result over time.

The downward trend in effect sizes is also seen if we only analyze the first reported statistical test in each paper, going from an average correlation of `r mean(filter(datMetaFirst, year < 1990)$r)` from 1985 - 1989 (inclusive) to a mean reported correlation of `r mean(filter(datMetaFirst, year > 2008)$r)` from 2009 to 2013. According to the results of the meta-regression including only the first APA reported result in each paper, there is an estimated yearly decrease of `r abs(modFirst$b[2])` (95% CI [`r modFirst$ci.lb[2]`, `r modFirst$ci.ub[2]`]) in $Fisher_z$ units. Over the 28 year time period included in this database, this represents an estimated decrease of `r changeOverTimeFirst` in correlation coefficient terms.

However, looking only at the largest reported effect in each paper, this trend is no longer apparent. Results from the multilevel meta-regression show an estimated yearly increase of  `r abs(modLargest$b[2])` (95% CI [`r options(digits=4); modLargest$ci.lb[2]`, `r modLargest$ci.ub[2]`]) `r  options(digits=3)` in $Fisher_z$ units, or alternatively an estimated increase of r = `r round(changeOverTimeLargest, 2)` between 1985 and 2013. This result may be driven by the increasing number of statistical tests reported per article (see Figure [n tests]). If more analyses are being performed over time (and assuming that the tests performed are at least somewhat independent), selecting the largest reported effect out of each article should show an increased average effect size on the basis of sampling variability alone. Furthermore, the effect sizes seen when selecting the largest effect in each paper (with a mean r of `r mean(datMetaLargest$r)`) are much larger than have previously been seen in examinations of effect sizes in psychological research (e.g., Bosco et al., 2015; Haase et al., 1982; Paterson et al., 2015; Richard et al., 2003), suggesting that this selection method does not  reliably identify the effects of substantive interest. Nonetheless, these exploratory analyses reinforce the fact that the results seen across all reported effect sizes should be interpreted carefully. Future research is necessary before any strong conclusions are drawn about trends in the size of the effects of substantive interest in psychological research.

```{r eval=FALSE, include=FALSE}
# calculating the min significant effect for each test 
minSigTestStat <- ifelse(datMeta$Statistic == F, qf(.05 , datMeta$df1, datMeta$df2, lower.tail = F), 
        ifelse(datMeta$Statistic == F, qt(.95, datMeta$df2, lower.tail = F),
              qnorm(.05, 0, sd = datMeta$SEz, lower.tail = F ))) 

datMeta$minimumSignificantR <- esComp(x = minSigTestStat, df1 = datMeta$df1, df2 =  datMeta$df2, esType = datMeta$Statistic)

minSignificant <- datMeta %>% 
    group_by(year) %>%
    summarise(MeanMinSigEffect = mean(minimumSignificantR), MedianMinSigEffect = median(minimumSignificantR), sd(minimumSignificantR), n = n()) %>%
  gather(average_type, average, MeanMinSigEffect:MedianMinSigEffect, factor_key = T)


trendsByTypeL <- datMetaLargest %>% 
    group_by(Statistic, year) %>%
    summarise("Mean ES" = mean(z), "SD first ES" = sd(z), n = n())

averageEffectL <- ggplot(trendsByTypeL, aes(x = year, y = `Mean ES`, group = Statistic, fill = Statistic, colour = Statistic)) + geom_line() + ylim(0,1.3) + geom_point() + ylab("Mean effect size (Fisher z)") + xlab("Year") # + theme(legend.position="none")

averageEffectL

### Plotting the minimum effect size that would be statistically significant 

ggplot(data = minSignificant, aes(year, average, group = average_type, colour = average_type)) + geom_line() + ylim(0,.15)

```


```{r fig.width = 16, fig.height=22}
predsForPlot <-  data.frame(pred = transf.ztor(intervals(mod)$fixed[1,2] + (intervals(mod)$fixed[2,2] * (1984:2014 - mean(datMeta$year)))))
predsForPlot$ci.lb <- transf.ztor(intervals(mod)$fixed[1,1] + (intervals(mod)$fixed[2,1] * (1984:2014 - mean(datMeta$year))))
predsForPlot$ci.ub <- transf.ztor(intervals(mod)$fixed[1,3] + (intervals(mod)$fixed[2,3] * (1984:2014 - mean(datMeta$year))))

predsForPlot <- data.frame(predsForPlot)
predsForPlot$year <- 1984:2014

 predsForPlotL <- predict(modLargest, 1984:2014 - mean(datMetaLargest$year), transf = transf.ztor)
 predsForPlotL <- data.frame(predsForPlotL)
  predsForPlotL$year <- 1984:2014
 
 predsForPlotF <- predict(modFirst, 1984:2014 - mean(datMetaFirst$year), transf = transf.ztor)
 predsForPlotF <- data.frame(predsForPlotF)
  predsForPlotF$year <- 1984:2014


plotMod <- ggplot(data = datMeta, aes(y = r, x = year, group = year, colour = journal)) + geom_jitter(alpha = .03, width = .4, height = 0)+ ylab("Correlation coefficient") + xlab("Year") + guides(colour = guide_legend(override.aes = list(alpha = 1)))+ geom_line( data = predsForPlot, aes(x = year, y= ci.lb), inherit.aes = F, linetype = 2)+ geom_line( data = predsForPlot, aes(x = year, y= ci.ub), inherit.aes = F, linetype = 2) + geom_line(data = predsForPlot, aes(x = year, y = pred), inherit.aes = F) + #geom_ribbon( data = predsForPlot, aes(x = year, ymin = ci.lb, ymax = ci.ub), inherit.aes = F, alpha = .1) + 
  theme(legend.position="none") + ggtitle("All data")

plotModLargest <- ggplot(data = datMetaLargest, aes(y = r, x = year, group = year, colour = journal))+ geom_jitter(alpha = .15, width = .4, height = 0)+ ylab("Correlation coefficient") + xlab("Year") + guides(colour = guide_legend(override.aes = list(alpha = 1)))+ geom_line( data = predsForPlotL, aes(x = year, y= ci.lb), inherit.aes = F, linetype = 2)+ geom_line( data = predsForPlotL, aes(x = year, y= ci.ub), inherit.aes = F, linetype = 2) + geom_line(data = predsForPlotL, aes(x = year, y = pred), inherit.aes = F) + # geom_ribbon( data = predsForPlotL, aes(x = year, ymin = ci.lb, ymax = ci.ub), inherit.aes = F, alpha = .1)+
  ggtitle("Largest reported effect")+ theme(legend.position="bottom")

plotModFirst <- ggplot(data = datMetaFirst, aes(y = r, x = year, group = year, colour = journal))+ geom_jitter(alpha = .15, width = .4, height = 0)+ ylab("Correlation coefficient") + xlab("Year") + guides(colour = guide_legend(override.aes = list(alpha = 1)))+ geom_line( data = predsForPlotF, aes(x = year, y= ci.lb), inherit.aes = F, linetype = 2)+ geom_line( data = predsForPlotF, aes(x = year, y= ci.ub), inherit.aes = F, linetype = 2) + geom_line(data = predsForPlotF, aes(x = year, y = pred), inherit.aes = F) + # geom_ribbon( data = predsForPlotF, aes(x = year, ymin = ci.lb, ymax = ci.ub), inherit.aes = F, alpha = .1) + 
  theme(legend.position="none") + ggtitle("First reported effect")

plot_grid(plotMod, plotModFirst, plotModLargest, nrow =  3, rel_widths = c(1, 1, 1), rel_heights = c(1,1,1.2))

```

Figure [jitter] A jitter plot of the reported effect sizes in this dataset plotted over time, with an overlaid  multilevel meta-regression plot (see Table [nice MLME sum datMeta] for model parameters, the plotted output has been converted to correlation coefficient units).

Overall, this analysis suggests that there has been a substantial decrease in the average magnitude of effects reported in psychology research papers from 1985 to 2013. However, the degree to which this result is reflective of a decrease in the size of the focal or main effects under study in psychology remains an open question. Whether these results are representative of focal analyses or not, there is little indication that effect sizes are notably rising over time. Together with the results of my previous meta-analysis, this finding suggests that the average statistical power of psychology research has remained consistently low across time [meta-analysis], and highlights the need for researchers to consider what effect sizes they expect to see during the planning of their studies in order to avoid unknowingly performing underpowered or imprecise research.

## Supplementary materials

### Supplementary materials 1. Effect size conversions
All statistical tests extracted were transformed into correlation coefficients as follows, using the methods reported in {Open Science Collaboration, 2015 #611}. 

t statistics:

$r\ =\ \sqrt{\frac{t_{obs}^2\times(1 / df)}{(t_{obs}^2 / df) + 1}}$

Where $t_{obs}$ is the observed t statistic and $df$ is the degrees of freedom of the t test. 

F statistics:

$r\  =\ \sqrt{\frac{F_{obs}\times(df_1 / df_2)}{F_{obs} \times (df_1 / df_2) + 1}} \times \sqrt{\frac{1}{df_1}}$

Where $F_{obs}$ is the observed F statistic and $df_1$ is the degrees of freedom of the numerator and $df_2$ is degrees of freedom of the denominator.  

Chi square statistics:

$r\ =\ \sqrt{\frac{\chi^2_{obs}}{df + 2}}$

Where $\chi^2_{obs}$ is the observed $\chi^2$ statistic and $df$ is the associated degrees of freedom.

All values were then transformed into fisher Z transformed correlation coefficients using:

$z\ = \frac 12 \times \ln{\Big(\frac{1 + r}{1 - r}}\Big)$

Standard errors for these statistics when derived from F tests with denominator degrees of freedom of 1, t tests, and correlation coefficients were estimated as:

$\sigma_{\hat{z}}\ =\ \frac{1}{\sqrt{n - 3}}$ 


### Supplementary materials 2. Additional exploratory analyses

##### Ignoring sampling variance and including all data

An alternative approach to analyzing this data is to ignore the sampling variances of each reported effect size and analyse the data using a typical multilevel model with a main effect for year and random effects for article and journal. In this approach, we no longer include random effects at the individual effect-size level, as we could not estimate both residual and effect level variances (both being random effects at the individual effect-size level).

$ES_j = \gamma_0 + \gamma_1Year + u_{article} + u_{journal} + e_{ij}$

```{r}
options(digits=5, scipen = 5)
# modNoWeights <- lme(z ~ 1 + I(year - mean(year)), random = ~ 1 |  journal / article, data =  datComplete)
# write_rds(modNoWeights, "data/modNoWeights.RDS")
modNoWeights <- read_rds("data/modNoWeights.RDS")

```

```{r}
kable(
data_frame(" " = c("Intercept", "Year",rep(NA,4)), 
  Estimate = c(modNoWeights$coefficients$fixed, rep(NA,4)), 
           "95% CI LB" = c(intervals(modNoWeights)$fixed[,1],rep(NA, 4)),
           "95% CI UB" = c(intervals(modNoWeights)$fixed[,3],rep(NA, 4)),
           SE = c(diag(sqrt(summary(modNoWeights)$varFix)), rep(NA, 4)),
           "p" = c("< .001", "< .001", rep(NA, 4)),
           "Random effects" = c(rep(NA, 2), 
                                paste0("Residual variance = ", round(as.numeric(VarCorr(modNoWeights))[5], 3), ", n = ", nrow(datMeta)),
                                paste0("Article variance = ", round(as.numeric(VarCorr(modNoWeights))[4], 3), ", n = ",length(unique(datMeta$article))),
                                paste0("Journal variance = ", round(as.numeric(VarCorr(modNoWeights))[2], 3), ", n = ", length(unique(datMeta$journal))),
                                paste0("QE(", nrow(datMeta) - 1, ") = ", round(sum(residuals(modNoWeights, type="pearson")^2),2), 
                                       ", p = < .001") 
                                )) 
)
```

Using this approach lead to a less extreme year by year difference with the estimated change per year of `r transf.rtoz( modNoWeights$coefficients$fixed[2])`. Otherwise, this model shows similar results.


##### Fixed effects for statistic type 

It also is possible to estimate fixed effects for each of the included statistical tests (F, r and t tests). This analysis is otherwise identical to that presented in the main text (i.e., it examines the `r nrow(datMeta)` effects for which valid standard errors were developed and accounts for the imprecision in each estimated effect size).

$ES_j = \gamma_0 + \gamma_1Year + \gamma_2t_{dummy} + \gamma_3r_{dummy} + u_{effect} + u_{article} + u_{journal} + e_{j}$

```{r}
# modFEST <- lme(z ~ 1 + I(year - mean(year)) + Statistic, random = ~ 1 |  journal / article / id,  weights = varFixed(~SEz^2), data = datMeta, control = lmeControl(sigma = 1, apVar = T))
# write_rds(modFEST, "data/modFEST")
modFEST <- read_rds("data/modFEST")

```


```{r}
kable(
data_frame(" " = c("Intercept", "Year", "r" ,"t" , rep(NA,4)), 
  Estimate = c(modFEST$coefficients$fixed, rep(NA,4)), 
           "95% CI LB" = c(intervals(modFEST)$fixed[,1],rep(NA, 4)),
           "95% CI UB" = c(intervals(modFEST)$fixed[,3],rep(NA, 4)),
           SE = c(diag(sqrt(summary(modFEST)$varFix)), rep(NA, 4)),
           "p" = c("< .001", "< .001","< .001", "< .001", rep(NA, 4)),
           "Random effects" = c(rep(NA, 4), 
                                paste0("Effect variance = ", round(as.numeric(VarCorr(modFEST))[6], 3), ", n = ", nrow(datMeta)),
                                paste0("Article variance = ", round(as.numeric(VarCorr(modFEST))[4], 3), ", n = ",length(unique(datMeta$article))),
                                paste0("Journal variance = ", round(as.numeric(VarCorr(modFEST))[2], 3), ", n = ", length(unique(datMeta$journal))),
                                paste0("QE(", nrow(datMeta) - 1, ") = ", round(sum(residuals(modFEST, type="pearson")^2),2), 
                                       ", p = < .001") 
                                ))
)
```

Including fixed effects for statistic type (i.e., a dummy coded variable for r and t tests, leaving F tests as the comparison group) lead to a negligible change in the estimated change over time (with the estimated change per year increasing by `r fixef(modFEST)[2] - fixef( mod)[2]`), but there were small but noticeable effects for effect size type (for t, $\gamma_2$ = `r fixef(modFEST)[4]`, 95% CI [`r intervals(modFEST)$fixed[4,c(1,3)]`], and for r $\gamma_3$ = `r fixef(modFEST)[3]`, 95% CI [`r intervals(modFEST)$fixed[3,c(1,3)]`]), with F as the comparison or baseline group (with an intercept of `r fixef(modFEST)[1]`, 95% CI [`r intervals(modFEST)$fixed[1,c(1,3)]`]).

##### Allowing slopes to vary by effect size type

Another question we investigated is whether the change in effect sizes over time may differ for the different types of statistical test reported. In order to examine this possibility we included both main fixed effects for statistical test type and interactions between year and test statistic type.

$ES_j = \gamma_0 + \gamma_1Year + \gamma_2t_{dummy} + \gamma_3r_{dummy} + \gamma_4t_{dummy} \times Year + \gamma_5r_{dummy}\times Year  + u_{effect} + u_{article} + u_{journal} + e_{effect}$

```{r}
# modFESTSlopes <- lme(z ~ 1 + centeredYear + centeredYear*Statistic + Statistic , random = ~ 1 |  journal / article / id,  weights = varFixed(~SEz^2), data = datMeta, control = lmeControl(sigma = 1, apVar = T))
# write_rds(modFESTSlopes, "data/modFESTSlopes")
modFESTSlopes <- read_rds("data/modFESTSlopes")

```


```{r}
kable(
data_frame(" " = c("Intercept", "Year", "r" ,"t" , "r * year", "t * year", rep(NA,4)), 
  Estimate = c(modFESTSlopes$coefficients$fixed, rep(NA,4)), 
           "95% CI LB" = c(intervals(modFESTSlopes)$fixed[,1],rep(NA, 4)),
           "95% CI UB" = c(intervals(modFESTSlopes)$fixed[,3],rep(NA, 4)),
           SE = c(diag(sqrt(summary(modFESTSlopes)$varFix)), rep(NA, 4)),
           "p" = c("< .001", "< .001","< .001", "< .001", 0.336, 0.939, rep(NA, 4)),
           "Random effects" = c(rep(NA, 6), 
                                paste0("Effect variance = ", round(as.numeric(VarCorr(modFESTSlopes))[6], 3), ", n = ", nrow(datMeta)),
                                paste0("Article variance = ", round(as.numeric(VarCorr(modFESTSlopes))[4], 3), ", n = ",length(unique(datMeta$article))),
                                paste0("Journal variance = ", round(as.numeric(VarCorr(modFESTSlopes))[2], 3), ", n = ", length(unique(datMeta$journal))),
                                paste0("QE(", nrow(datMeta) - 1, ") = ", round(sum(residuals(modFESTSlopes, type="pearson")^2),2), 
                                       ", p = < .001") 
                                ))
)
```

Allowing slopes to vary by statistic type (i.e., including an interaction between dummy coded binaries for r and t statistics) led to negligible estimated effects interaction effects. The interaction effect for correlations, $\gamma_3$, was `r fixef(modFESTSlopes)[5]`, 95% CI [`r intervals(modFESTSlopes)$fixed[5,c(1,3)]`]. The interaction effect for t statistics, $\gamma_4$, was `r fixef(modFESTSlopes)[6]`, 95% CI [`r intervals(modFESTSlopes)$fixed[6,c(1,3)]`]. $\gamma_1$,  which now represents the estimated change per year in effect sizes for F statistics, was estimated as `r fixef(modFESTSlopes)[2]`, 95% CI [`r intervals(modFESTSlopes)$fixed[6,c(1,3)]`]. This is only `r fixef(modFESTSlopes)[2] - fixef( mod)[2]` units more extreme than the $\gamma_1$ estimated when including main effects or interaction effects for test statistic types.

##### Allowing slopes to vary by journal 

An alternative approach to analyzing this data set is to allow the relationship between effect size and time to vary by journal. 

$ES_j = \gamma_{0} + \gamma_{1}Year + u_{effect} + u_{article} + u_{journal} + u_{jounral_2} \times Year + e_{effect}$

```{r}
# datMeta$centeredYear <- datMeta$year - mean(datMeta$year)
# slopesVByJ <- lme(z ~ 1 + centeredYear, random = list ( ~ 1 + centeredYear |  journal, ~ 1 | article , ~ 1 | id ),  weights = varFixed(~SEz^2), data = datMeta, control = lmeControl(sigma = 1, apVar = T))
# write_rds(slopesVByJ, "data/slopesVByJ")
slopesVByJ <- read_rds("data/slopesVByJ")

```

```{r}
kable(
data_frame(" " = c("Intercept", "Year",rep(NA,5)), 
  Estimate = c(slopesVByJ$coefficients$fixed, rep(NA,5)), 
           "95% CI LB" = c(intervals(slopesVByJ)$fixed[,1],rep(NA, 5)),
           "95% CI UB" = c(intervals(slopesVByJ)$fixed[,3],rep(NA, 5)),
           SE = c(diag(sqrt(summary(slopesVByJ)$varFix)), rep(NA, 5)),
           "p" = c("< .001", "< .001", rep(NA, 5)),
           "Random effects" = c(rep(NA, 2), 
                                paste0("Effect variance = ", round(as.numeric(VarCorr(slopesVByJ))[7], 3), ", n = ", nrow(datMeta)),
                                paste0("Article variance = ", round(as.numeric(VarCorr(slopesVByJ))[5], 3), ", n = ",length(unique(datMeta$article))),
                                paste0("Journal variance = ", round(as.numeric(VarCorr(slopesVByJ))[2], 3), ", n = ", length(unique(datMeta$journal))),
                                 paste0("Slope variance = ", round(as.numeric(VarCorr(slopesVByJ))[3], 3), ", n = ", length(unique(datMeta$journal))),
                                paste0("QE(", nrow(datMeta) - 1, ") = ", round(sum(residuals(slopesVByJ, type="pearson")^2),2), 
                                       ", p = < .001") 
                                ))
)
```

Including random slopes for journal lead to almost no change in the overall estimated change per year ($\gamma_1$ = 
`r fixef(mod)[2]`, a change of just `r fixef(mod)[2] - fixef(slopesVByJ)[2]`), and a very small estimated slope variance  of `r options(digits=6, scipen = 5); round( intervals(slopesVByJ)$reStruc$journal[2,2]^2, 6)`, 95% CIs `r intervals(slopesVByJ)$reStruc$journal[2,c(1,3)]^2`. This suggests that the effect size change per year is relatively stable across journals, although the sampling plan used here (only including 5 APA journals), may limit the generalizability of these results outside of this sample.
